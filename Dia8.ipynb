{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Día 8\n",
    "\n",
    "Hemos visto el **perceptron** y el **clasificador logístico**. Son sistemas de aprendizaje supervisado.\n",
    "\n",
    "**Máquinas de vector de soporte (svm)** que permiten adaptar su forma a nuestros datos y usa un kernel que no es el lineal usado por los anteriores como el rbf que es gausiano. La contrapartida es que son más difíciles de ajustar. Solo los vectores junto a la frontera son los que usamos para obtener el modelo.\n",
    "\n",
    "\n",
    "**Arboles de decisión** son más fáciles de interpretar y el coste computacional es bajo. Las fronteras son paralelas a los ejes\n",
    "\n",
    "**Random Forest** son muchos árboles y clasificamos o votamos y el que tiene mayoría es el elegido.\n",
    "\n",
    "**K-vecinos ** no tiene entrenamiento y consiste en busar el números de vecinos que puede ser aproximado o exacto pero en este último caso tiene un alto coste computacional.\n",
    "\n",
    "No hay  ningún modelo que se comporte bien con cualquier tipo de datos\n",
    "\n",
    "Por último vimos los sistemas de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizaje no supervisado. Agrupamiento (Clustering).\n",
    "\n",
    "En este caso al no tener la solución o salida, podemos agrupar para clasificar patrones.\n",
    "\n",
    "Ahora no hay concepto de clase de salida.\n",
    "\n",
    "Vamos a ver k means y dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means\n",
    "\n",
    "Agrupa nuestros datos de forma muy sencilla, similar a los algoritmos e-m (expectation - maximitation)\n",
    "\n",
    "Los centroides son el prototipo, es decir, representante.\n",
    "\n",
    "El algoritmo tiene 2 etapas:\n",
    "* Seleccionar K muestras al azar y clasifico las muestras en función de esos centroides elegidos aleatoriamente\n",
    "* Asignamos a cada muestra una una clase en función del centroide elegido al azar\n",
    "* Calculo los nuevos centroides\n",
    "\n",
    "Sigo así hasta que el centroide no cambia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "import numpy as np\n",
    "    \n",
    "def plot_classif(X, X0, classif):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,0], X[:,1],marker='x', c=classif)\n",
    "    plt.scatter(X0[:,0], X0[:,1],marker='o', c=sorted(np.unique(classif)))\n",
    "    plt.title('Muestras y centroides')\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:4]\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "# Inicialización: elegimos K centroides\n",
    "n_iters = 5\n",
    "K = 2\n",
    "idx = np.random.choice(X.shape[0], K, replace=False)\n",
    "\n",
    "X0 = X[idx, :]\n",
    "\n",
    "for n in range(n_iters):\n",
    "    # Cálculo de distancias y clasificación\n",
    "    d_sq = np.zeros([X.shape[0], K])\n",
    "    for i_k in range(K):\n",
    "        d_sq[:,i_k] = np.sum((X-X0[i_k,:])**2, axis=1)\n",
    "    \n",
    "    classif = np.argmin(d_sq, axis=1)\n",
    "    \n",
    "    plot_classif(X, X0, classif)\n",
    "    plt.title('Tras clasificación {}'.format(n))\n",
    "    # Fase 2: reasignación del centroide\n",
    "    for i_k in range(K):\n",
    "        X0[i_k,:] = np.mean(X[classif==i_k], axis=0)\n",
    "    plot_classif(X, X0, classif)\n",
    "    plt.title('Tras cálculo centroide {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos la implemetacion de k menas de sklearn\n",
    "\n",
    "K means se comporta bien con datos homogeneos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, _ = make_blobs(n_samples=150,\n",
    "    n_features=2,\n",
    "    centers=3,\n",
    "    cluster_std=0.5,\n",
    "    shuffle=True,\n",
    "    random_state=0)\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=3,\n",
    "    init='random',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    tol=1e-04,\n",
    "    random_state=0)\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ejercicio\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300,\n",
    "    n_features=2,\n",
    "    centers=6,\n",
    "    cluster_std=0.5,\n",
    "    shuffle=True,\n",
    "    random_state=0)\n",
    "\n",
    "inertias_ = []\n",
    "Ks = range(1, 11)\n",
    "for k in Ks:\n",
    "    km = KMeans(n_clusters=k,\n",
    "        init='random',\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        tol=1e-04,\n",
    "        random_state=0, verbose=1)\n",
    "    \n",
    "    y_km = km.fit_predict(X)\n",
    "    inertias_.append(km.inertia_)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(Ks, inertias_)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Evolución de inercia en función de K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN (agrupamiento por densidad)\n",
    "\n",
    "En agrupamiento tenemos el agrupamiento por densidad que es útil cuando hay muchas dimensiones\n",
    "\n",
    "Funciona en dos etapas:\n",
    "* Cada muestra se analiza viendo sus vecinos y se clasifica como nucleo, frontera y ruido\n",
    "* Agrupa en clusters nucleo y frontera. El ruido lo descarta.\n",
    "\n",
    "Buscamos en cada muestra buscamos los vecinos dentro de un radio (epsilon) y contamos los vecinos con ello lo clasificamos como nucleo, frontera o ruido en función si tiene o no el número mínimo de puntos.\n",
    "\n",
    "La operación de busque de vecinos es costosa computacionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparación con moons entre Kmeans y DBSCAN\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "X, _ = make_moons(n_samples=200, shuffle=True,noise=0.05)\n",
    "\n",
    "K= 2\n",
    "\n",
    "km = KMeans(n_clusters=K,n_init=10)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "db = DBSCAN(eps=0.2,min_samples=5,metric='euclidean')\n",
    "y_db = db.fit_predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y_km)\n",
    "plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],c=range(K),marker='s')\n",
    "plt.title('Moons')\n",
    "\n",
    "plt.figure()\n",
    "validas = y_db!=-1\n",
    "no_validas = y_db == -1\n",
    "#plt.scatter(X[:,0], X[:,1], c = y_db)\n",
    "plt.scatter(X[validas,0], X[validas,1],c=y_db[validas])\n",
    "plt.scatter(X[no_validas,0], X[no_validas,1],c=y_db[no_validas])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing different clustering algorithms on toy datasets\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "clustering_names = [\n",
    "    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',\n",
    "    'SpectralClustering', 'Ward', 'AgglomerativeClustering',\n",
    "    'DBSCAN', 'Birch']\n",
    "\n",
    "plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "datasets = [noisy_circles, noisy_moons, blobs, no_structure]\n",
    "for i_dataset, dataset in enumerate(datasets):\n",
    "    X, y = dataset\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # create clustering estimators\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n",
    "    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',\n",
    "                                           connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(n_clusters=2,\n",
    "                                          eigen_solver='arpack',\n",
    "                                          affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=.2)\n",
    "    affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
    "                                                       preference=-200)\n",
    "\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\", n_clusters=2,\n",
    "        connectivity=connectivity)\n",
    "\n",
    "    birch = cluster.Birch(n_clusters=2)\n",
    "    clustering_algorithms = [\n",
    "        two_means, affinity_propagation, ms, spectral, ward, average_linkage,\n",
    "        dbscan, birch]\n",
    "\n",
    "    for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "        # predict cluster memberships\n",
    "        t0 = time.time()\n",
    "        algorithm.fit(X)\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        # plot\n",
    "        plt.subplot(4, len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "        if hasattr(algorithm, 'cluster_centers_'):\n",
    "            centers = algorithm.cluster_centers_\n",
    "            center_colors = colors[:len(centers)]\n",
    "            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "        plt.xlim(-2, 2)\n",
    "        plt.ylim(-2, 2)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se puede usar en visión artificial, agrupando los pixeles de los objetos que son del mismo color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad\n",
    "\n",
    "Hay dimensiones que no aportan nada y meten ruido en los algoritmos de agrupamiento\n",
    "\n",
    "https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad\n",
    "\n",
    "![](http://www.cimat.mx/sites/default/files/Unidad_Mty/expo1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from plot_regions import plot_decision_regions\n",
    "from matplotlib import pyplot as plt\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "lr_l1 = LogisticRegression(C=0.1, penalty= 'l1',random_state=0)\n",
    "lr_l2 = LogisticRegression(C=0.1, penalty= 'l2',random_state=0)\n",
    "\n",
    "lr_l1.fit(X_train_std, y_train)\n",
    "lr_l2.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"Coefs l1: {}\".format(lr_l1.coef_))\n",
    "print(\"Coefs l2: {}\".format(lr_l2.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**resultado:**\n",
    "\n",
    "Coefs l1: [[ 0.          0.25820196 -1.84055701  0.        ]\n",
    " [ 0.         -0.52154555  0.          0.        ]\n",
    " [ 0.          0.          0.          1.56924881]]\n",
    "\n",
    "Coefs l2: [[-0.48205867  0.65320727 -0.80794452 -0.75352293]\n",
    " [-0.00882554 -0.68462103  0.14644841 -0.1107326 ]\n",
    " [ 0.4163131   0.08800633  0.66162765  0.86172684]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Con los random forest podemos reducir la dimensionalidad si tenemos un número de arboles elevado\n",
    "\n",
    "Un random forest nos genera un vector con la importancia de cada característica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=5000) #uso 5000 arboles\n",
    "rf.fit(X_train, y_train)\n",
    "print ('Importancia: {}'.format(rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado me dice la importancia de las caracteristicas\n",
    "\n",
    "**Importancia: [ 0.10923575  0.02356837  0.45524838  0.4119475 ]**\n",
    "\n",
    "Las características más importantes son la 3 y 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos dos grupos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "f_datos = [load_iris, load_breast_cancer]\n",
    "\n",
    "for f_load in f_datos:\n",
    "    datos = f_load()\n",
    "\n",
    "    X = datos.data\n",
    "    y = datos.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=5000) #uso 5000 arboles\n",
    "    rf.fit(X_train, y_train)\n",
    "    print ('Importancia: {}'.format(rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**\n",
    "\n",
    "Para Iris: \n",
    "\n",
    "Importancia: [ 0.10737728  0.02310471  0.45219645  0.41732156]\n",
    "\n",
    "Para Cancer Breast:\n",
    "\n",
    "Importancia: [ 0.03403341  0.01514795  0.046186    0.04017112  0.00585401  0.01253403\n",
    "  0.05426708  0.12143118  0.00481444  0.00395686  0.01530623  0.00533229\n",
    "  0.01356237  0.04097715  0.00406805  0.00552294  0.00708099  0.00569799\n",
    "  0.00434034  0.00539303  0.1019194   0.01688158  0.1283404   0.10346233\n",
    "  0.01281556  0.0180733   0.03368638  0.11983818  0.0122723   0.0070331 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importancia = [ 0.03403341,0.01514795,0.046186,0.04017112,0.00585401,0.01253403,0.05426708,0.12143118,0.00481444,0.00395686,0.01530623,0.00533229,0.01356237,0.04097715,0.00406805,0.00552294,0.00708099,0.00569799,0.00434034,0.00539303,0.1019194,0.01688158,0.1283404,0.10346233,0.01281556,0.0180733,0.03368638,0.11983818,0.0122723,0.0070331 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03403341,\n",
       " 0.01514795,\n",
       " 0.046186,\n",
       " 0.04017112,\n",
       " 0.00585401,\n",
       " 0.01253403,\n",
       " 0.05426708,\n",
       " 0.12143118,\n",
       " 0.00481444,\n",
       " 0.00395686,\n",
       " 0.01530623,\n",
       " 0.00533229,\n",
       " 0.01356237,\n",
       " 0.04097715,\n",
       " 0.00406805,\n",
       " 0.00552294,\n",
       " 0.00708099,\n",
       " 0.00569799,\n",
       " 0.00434034,\n",
       " 0.00539303,\n",
       " 0.1019194,\n",
       " 0.01688158,\n",
       " 0.1283404,\n",
       " 0.10346233,\n",
       " 0.01281556,\n",
       " 0.0180733,\n",
       " 0.03368638,\n",
       " 0.11983818,\n",
       " 0.0122723,\n",
       " 0.0070331]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 9, 14, 18,  8, 11, 19, 15, 17,  4, 29, 16, 28,  5, 24, 12,  1, 10,\n",
       "         21, 25, 26,  0,  3, 13,  2,  6, 20, 23, 27,  7, 22]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "importancia_np = np.matrix(importancia)\n",
    "importancia_np.argsort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
