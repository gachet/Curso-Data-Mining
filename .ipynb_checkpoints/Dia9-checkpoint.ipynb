{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dia 9\n",
    "\n",
    "Reutilización de código\n",
    "\n",
    "Al ejecutar un módulo lo ejecuto con en \\__name\\__ = \\__main\\__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Por lo tanto al importar un módulo lo que no quiero que se ejecute lo pongo como:\n",
    "\n",
    "if __name__ = '__main__':\n",
    "    #codigp que no se ejecuta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es común encontrar trozos de código de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA: análisis de componentes principales\n",
    "\n",
    "El análisis por componentes principales es una técnica de transformación lineal basada en el análisis de la matriz de correlación.\n",
    "\n",
    "PCA es una técnica no supervisada (no hay datos de salida) y sirve para:\n",
    "* Reducción de dimensionalidad\n",
    "* Análisis exploratorio de datos\n",
    "* Reducción de ruido\n",
    "* Correspondencia de patrones\n",
    "\n",
    "Obtenemos un nuevo conjunto de ejes que nos da más \n",
    "\n",
    "Más información:\n",
    "* https://es.wikipedia.org/wiki/An%C3%A1lisis_de_componentes_principales\n",
    "* https://www.uoc.edu/in3/emath/docs/Componentes_principales.pdf\n",
    "\n",
    "Paso 1 - Es necesario normalizar los datos y centrados a cero\n",
    "\n",
    "Paso 2 - Obtener la matriz de covarianza\n",
    "\n",
    "![](http://www.sthda.com/sthda/RDoc/figure/factor-analysis/factominer-principal-component-analysis-individuals-factor-map-color-by-groups-factoextra-data-mining-1.png)\n",
    "\n",
    "Con PCA puede reducir las dimensiones y tratar de ver normalizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "datos = load_iris()\n",
    "X = datos.data\n",
    "#normalizamos\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "#matriz de covarianza\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "var_explained_ratio = eigen_vals/np.sum(eigen_vals)\n",
    "eigen_vals\n",
    "eigen_vecs\n",
    "var_explained_ratio #ratio de como explica mi  muestra cada eje\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_std[:,0], X_std[:,1])\n",
    "plt.plot([0, eigen_vecs[0,0]],[0, eigen_vecs[1,0]],c='r',label=\"{}\".format(eigen_vals[0]))\n",
    "plt.plot([0, eigen_vecs[0,1]],[0, eigen_vecs[1,1]],c='k',label=\"{}\".format(eigen_vals[1]))\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado**\n",
    "\n",
    "[ 2.93035378  0.92740362  0.14834223  0.02074601]\n",
    "\n",
    "[[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
    " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
    " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
    " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
    " \n",
    "[ 0.72770452  0.23030523  0.03683832  0.00515193]\n",
    "\n",
    "![](files/images/pca.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_std)\n",
    "\n",
    "X_pca = pca.transform(X_std)\n",
    "X_2d = X_pca[:,0:2]\n",
    "plt.figure()\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/pca_2d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ahora podemos aplicar un clasificador de esas dos dimensiones por ejemplo SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(X_std, datos.target)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "y_p = svm.predict(X_test_std)\n",
    "score = accuracy_score(y_test, y_p)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**\n",
    "\n",
    "0.973684210526"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA (Análisis de Discriminantes Lineales)\n",
    "\n",
    "LDA es el hermano supervisado de PCA (que como hemos visto es una técnica no supervisada). \n",
    "\n",
    "En LDA utilizamos la información disponible en nuestro conjunto de entrenamiento para buscar la transformación lineal que asegura una mejor separabilidad de las clases (recordemos que PCA maximiza la varianza, independientemente de las clases que tengan asignadas nuestras muestras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow\n",
    "\n",
    "Google ha liberado tensorflow para visualizar datos  y más...\n",
    "\n",
    "**projector** http://projector.tensorflow.org/\n",
    "\n",
    "Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets para prácticas: https://www.kaggle.com/datasets\n",
    "\n",
    "En problemas complejos se suele aplicar primero PCA para reducir dimensiones.\n",
    "\n",
    "Otro más formal: https://www.innocentive.com/\n",
    "\n",
    "Otra: https://www.drivendata.org/\n",
    "\n",
    "Buenisima web con mucha información: http://www.kdnuggets.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión\n",
    "\n",
    "Usamos los notebooks de https://github.com/rasbt/python-machine-learning-book\n",
    "\n",
    "Ver notebook de [Regresion. capitulo 10](/notebooks/code/ch10/ch10.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**resto de notebook** [notebooks](/tree/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando son relaciones lineales, en sistemas lineales M * v = y --> v = inversa(M) * y\n",
    "\n",
    "Aplicamos la solución de mínimos cuadrados y lo único que se usa son operaciones matriciales.\n",
    "\n",
    "No hay más misterio que usar la ecuación de mínimos cuadrados, que tiene una solución. El único problema puede estar en matrices grandes y tiene un coste de computo.\n",
    "\n",
    "En matrices dispersas (son las que tienen muchos ceros), suelen aparecer en teoría de grafos.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Linear_least_squares2.png/800px-Linear_least_squares2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#muestras\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                 'housing/housing.data',\n",
    "                 header=None,\n",
    "                 sep='\\s+')\n",
    "\n",
    "df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', \n",
    "              'NOX', 'RM', 'AGE', 'DIS', 'RAD', \n",
    "              'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema con 14 dimensiones y 506 muestras.\n",
    "\n",
    "Es un problema pequeño.\n",
    "\n",
    "Minimos cuadrados para regresión puede usarse y funciona muy bien si no hay un número grande de muestras y no hay datos on-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos robustos no se ven afectados por los puntos outliers, al menos hasta un cierto número de ellos. Uno de ellos es RANSAC\n",
    "\n",
    "https://es.wikipedia.org/wiki/RANSAC\n",
    "\n",
    "RANCSAC se usar para comparar imagenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
